# ABSA-PyTorch

## Aspect Based Sentiment Analysis, PyTorch Implementations.
åŸºäºæ–¹é¢çš„æƒ…æ„Ÿåˆ†æï¼Œä½¿ç”¨PyTorchå®ç°ã€‚

```
![LICENSE](https://img.shields.io/packagist/l/doctrine/orm.svg)
[![Gitter](https://badges.gitter.im/ABSA-PyTorch/community.svg)](https://gitter.im/ABSA-PyTorch/community?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)
<!-- ALL-CONTRIBUTORS-BADGE:START - Do not remove or modify this section -->
[![All Contributors](https://img.shields.io/badge/all_contributors-7-orange.svg?style=flat-square)](#contributors-)
<!-- ALL-CONTRIBUTORS-BADGE:END -->
```

## Requirement

* pytorch >= 0.4.0
* numpy >= 1.13.3
* sklearn
* python 3.6 / 3.7
* transformers

To install requirements, run `pip install -r requirements.txt`.

## æ•°æ®é›†æ ¼å¼
$T$ ä»£è¡¨å¥å­ä¸­çš„çŸ­è¯­ï¼Œ1è¡¨ç¤ºPOSï¼Œ0è¡¨ç¤ºNEUï¼Œ-1è¡¨ç¤ºNEG
```buildoutcfg
$T$ is super fast , around anywhere from 35 seconds to 1 minute .
Boot time
1
$T$ would not fix the problem unless I bought your plan for $ 150 plus .
tech support
-1
No $T$ is included .
installation disk (DVD)
0
```

## æ¨¡å‹è®­ç»ƒå‚æ•°è®²è§£
```buildoutcfg
train.py
  --model_name MODEL_NAME
                        è¦ä½¿ç”¨çš„æ¨¡å‹ï¼Œæ¨¡å‹åœ¨modelsç›®å½•ä¸‹,ä¾‹å¦‚ aen_bertï¼Œbert_spcï¼Œlcf_bertç­‰
  --dataset DATASET     æ•°æ®é›†åç§°
  --optimizer OPTIMIZER
                        æ¨¡å‹è®­ç»ƒä¼˜åŒ–å™¨ï¼Œé»˜è®¤adam
  --initializer INITIALIZER
                        å‚æ•°åˆå§‹åŒ–æ–¹æ³•xavier_normal_,orthogonal_
  --learning_rate LEARNING_RATE
                        å­¦ä¹ ç‡BERTç”¨ 5e-5, 2e-5ï¼Œå…¶å®ƒç”¨æ¨¡å‹ç”¨ 1e-3
  --dropout DROPOUT     é»˜è®¤çš„droput 0.1
  --l2reg L2REG         weight_decay ,l2æ­£åˆ™ç³»æ•°
  --num_epoch NUM_EPOCH
                        éBERTç±»æ¨¡å‹ï¼Œè®­ç»ƒçš„epochéœ€è¦å¤šä¸€äº›
  --batch_size BATCH_SIZE
                        BERTæ¨¡å‹è¯·ä½¿ç”¨å¦‚ä¸‹batch_size 16, 32, 64
  --log_step LOG_STEP   æ¯å¤šå°‘stepè¿›è¡Œæ—¥å¿—è®°å½•
  --embed_dim EMBED_DIM
                        è¯åµŒå…¥æ—¶embeddingçš„ç»´åº¦ï¼Œé»˜è®¤300
  --hidden_dim HIDDEN_DIM
                        è®­ç»ƒæ—¶éšè—å±‚ç»´åº¦ï¼Œé»˜è®¤300
  --bert_dim BERT_DIM   bert dimï¼Œbertçš„è¯å‘é‡ç»´åº¦ï¼Œé»˜è®¤768
  --pretrained_bert_name PRETRAINED_BERT_NAME
                        ä½¿ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œä½¿ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œé»˜è®¤bert-base-chinese
  --pretrained_bert_cache_dir PRETRAINED_BERT_CACHE_DIR
                        ä½¿ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹ç¼“å­˜çš„ç›®å½•ï¼Œé»˜è®¤
  --embedding_file EMBEDDING_FILE
                        å¦‚æœä¸ä½¿ç”¨BERTï¼Œé‚£ä¹ˆè‡ªå®šä¹‰çš„é¢„è®­ç»ƒçš„è¯å‘é‡æ–‡ä»¶çš„ä½ç½®
  --max_seq_len MAX_SEQ_LEN
                        æœ€å¤§åºåˆ—é•¿åº¦
  --polarities_dim POLARITIES_DIM
                        ç±»åˆ«ç»´åº¦ï¼Œåˆ†å‡ ç±»,é»˜è®¤POSï¼ŒNEUï¼ŒNEG
  --hops HOPS           å¤šå°‘hopè®¾ç½®
  --device DEVICE       e.g. cuda:0
  --seed SEED           ç”¨äºé‡ç°ï¼Œéšæœºæ•°ç§å­
  --valset_ratio VALSET_RATIO
                        è®­ç»ƒé›†æ‹†åˆ†å‡ºéªŒè¯çš„æ¯”ä¾‹, åœ¨0å’Œ1ä¹‹é—´è®¾ç½®æ¯”ä¾‹ä»¥éªŒè¯,å¦‚æœä¸º0ï¼Œç”¨æµ‹è¯•é›†ä»£æ›¿éªŒè¯é›†
  --recreate_caches     é»˜è®¤Falseï¼Œæ˜¯å¦é‡æ–°ç”Ÿæˆæ•°æ®å¤„ç†çš„cacheæ–‡ä»¶
  --local_context_focus LOCAL_CONTEXT_FOCUS
                        æœ¬åœ°ä¸Šä¸‹æ–‡ç„¦ç‚¹æ¨¡å¼ï¼Œcdwæˆ–cdm
  --SRD SRD             è¯­ä¹‰ç›¸å¯¹è·ç¦»ï¼Œè¯·å‚é˜…LCF-BERTæ¨¡å‹çš„è®ºæ–‡
```

* å¯¹äºåŸºäºéBERTçš„æ¨¡å‹,
[GloVe pre-trained word vectors](https://github.com/stanfordnlp/GloVe#download-pre-trained-word-vectors) æ˜¯å¿…é¡»çš„, è¯·å‚è€ƒ [data_utils.py](./data_utils.py) æ›´å¤šç»†èŠ‚.
http://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.42B.300d.zip
ä¸‹è½½  glove.42B.300d.zipï¼Œè§£å‹ç¼© glove.42B.300d.txtï¼Œæ”¾åœ¨é¡¹ç›®æ ¹ç›®å½•

å¯¹æ¯”golveçš„ä¸­æ–‡è¯å‘é‡, æˆ–è€…å‚è€ƒutilsä¸­çš„train_word2vecå‡½æ•°ï¼Œè®­ç»ƒè‡ªå·±çš„è¯å‘é‡
https://github.com/Embedding/Chinese-Word-Vectors

## ä½¿ç”¨æ–¹å¼

### è®­ç»ƒ

```sh
python train.py --model_name bert_spc --dataset restaurant
python train.py --model_name aen_bert --dataset cosmetics --max_seq_len 200 --valset_ratio 0.1 --pretrained_bert_name bert-base-chinese --pretrained_bert_cache_dir model_cache --batch_size 16 --num_epoch 5
python train.py --model_name bert_spc --dataset cosmetics --max_seq_len 200 --valset_ratio 0.1 --pretrained_bert_name bert-base-chinese --pretrained_bert_cache_dir model_cache --batch_size 16 --num_epoch 5
python train.py --model_name lcf_bert --dataset cosmetics --max_seq_len 200 --valset_ratio 0.1 --pretrained_bert_name bert-base-chinese --pretrained_bert_cache_dir model_cache --batch_size 16 --num_epoch 3

#ä½¿ç”¨è‡ªå®šä¹‰çš„bert_td_lstm
python train.py --model_name bert_td_lstm --dataset cosmetics_as --do_train --valset_ratio 0.1 --learning_rate 1e-3 --max_seq_len 70 --batch_size 16 --num_epoch 15 --embed_dim 768
```


* æ‰€æœ‰å®ç°çš„æ¨¡å‹éƒ½åœ¨  [models directory](./models/).
* æŸ¥çœ‹ [train.py](./train.py) äº†è§£æ›´å¤šè®­ç»ƒå‚æ•°
* å‚è€ƒ [train_k_fold_cross_val.py](./train_k_fold_cross_val.py) kæŠ˜äº¤å‰éªŒè¯ã€‚

### æ¨ç†

* å‚è€ƒ [infer_example.py](./infer_example.py) ç”¨äºåŸºäºéBERTçš„æ¨¡å‹.
* å‚è€ƒ [infer_example_bert_models.py](./infer_example_bert_models.py) ç”¨äºåŸºäºBERTçš„æ¨¡å‹.

### æç¤º

* å¯¹äºåŸºäºéBERTçš„æ¨¡å‹ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸æ˜¯å¾ˆç¨³å®šã€‚
* åŸºäºBERTçš„æ¨¡å‹å¯¹å°æ•°æ®é›†çš„è¶…å‚æ•°ï¼ˆå°¤å…¶æ˜¯å­¦ä¹ ç‡ï¼‰æ›´æ•æ„Ÿï¼Œè¯·å‚é˜… [this issue](https://github.com/songyouwei/ABSA-PyTorch/issues/27).
* ä¸ºäº†é‡Šæ”¾BERTçš„çœŸæ­£æ€§èƒ½ï¼Œå¿…é¡»å¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œå¾®è°ƒã€‚


## Reviews / Surveys è®ºæ–‡

Qiu, Xipeng, et al. "Pre-trained Models for Natural Language Processing: A Survey." arXiv preprint arXiv:2003.08271 (2020). [[pdf]](https://arxiv.org/pdf/2003.08271)

Zhang, Lei, Shuai Wang, and Bing Liu. "Deep Learning for Sentiment Analysis: A Survey." arXiv preprint arXiv:1801.07883 (2018). [[pdf]](https://arxiv.org/pdf/1801.07883)

Young, Tom, et al. "Recent trends in deep learning based natural language processing." arXiv preprint arXiv:1708.02709 (2017). [[pdf]](https://arxiv.org/pdf/1708.02709)


## BERT-based models

### BERT-ADA ([official](https://github.com/deepopinion/domain-adapted-atsc))

Rietzler, Alexander, et al. "Adapt or get left behind: Domain adaptation through bert language model finetuning for aspect-target sentiment classification." arXiv preprint arXiv:1908.11860 (2019). [[pdf](https://arxiv.org/pdf/1908.11860)]

### BERR-PT ([official](https://github.com/howardhsu/BERT-for-RRC-ABSA))

Xu, Hu, et al. "Bert post-training for review reading comprehension and aspect-based sentiment analysis." arXiv preprint arXiv:1904.02232 (2019). [[pdf](https://arxiv.org/pdf/1904.02232)]

### ABSA-BERT-pair ([official](https://github.com/HSLCY/ABSA-BERT-pair))

Sun, Chi, Luyao Huang, and Xipeng Qiu. "Utilizing bert for aspect-based sentiment analysis via constructing auxiliary sentence." arXiv preprint arXiv:1903.09588 (2019). [[pdf](https://arxiv.org/pdf/1903.09588.pdf)]

### LCF-BERT ([lcf_bert.py](./models/lcf_bert.py)) ([official](https://github.com/yangheng95/LCF-ABSA))

Zeng Biqing, Yang Heng, et al. "LCF: A Local Context Focus Mechanism for Aspect-Based Sentiment Classification." Applied Sciences. 2019, 9, 3389. [[pdf]](https://www.mdpi.com/2076-3417/9/16/3389/pdf)

### AEN-BERT ([aen.py](./models/aen.py))

Song, Youwei, et al. "Attentional Encoder Network for Targeted Sentiment Classification." arXiv preprint arXiv:1902.09314 (2019). [[pdf]](https://arxiv.org/pdf/1902.09314.pdf)

### BERT for Sentence Pair Classification ([bert_spc.py](./models/bert_spc.py))

Devlin, Jacob, et al. "Bert: Pre-training of deep bidirectional transformers for language understanding." arXiv preprint arXiv:1810.04805 (2018). [[pdf]](https://arxiv.org/pdf/1810.04805.pdf)


## Non-BERT-based models

### MGAN ([mgan.py](./models/mgan.py))

Fan, Feifan, et al. "Multi-grained Attention Network for Aspect-Level Sentiment Classification." Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. 2018. [[pdf]](http://aclweb.org/anthology/D18-1380)

### AOA ([aoa.py](./models/aoa.py))

Huang, Binxuan, et al. "Aspect Level Sentiment Classification with Attention-over-Attention Neural Networks." arXiv preprint arXiv:1804.06536 (2018). [[pdf]](https://arxiv.org/pdf/1804.06536.pdf)

### TNet ([tnet_lf.py](./models/tnet_lf.py)) ([official](https://github.com/lixin4ever/TNet))

Li, Xin, et al. "Transformation Networks for Target-Oriented Sentiment Classification." arXiv preprint arXiv:1805.01086 (2018). [[pdf]](https://arxiv.org/pdf/1805.01086)

### Cabasc ([cabasc.py](./models/cabasc.py))

Liu, Qiao, et al. "Content Attention Model for Aspect Based Sentiment Analysis." Proceedings of the 2018 World Wide Web Conference on World Wide Web. International World Wide Web Conferences Steering Committee, 2018.

### RAM ([ram.py](./models/ram.py))

Chen, Peng, et al. "Recurrent Attention Network on Memory for Aspect Sentiment Analysis." Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. 2017. [[pdf]](http://www.aclweb.org/anthology/D17-1047)

### MemNet ([memnet.py](./models/memnet.py)) ([official](https://drive.google.com/open?id=1Hc886aivHmIzwlawapzbpRdTfPoTyi1U))

Tang, Duyu, B. Qin, and T. Liu. "Aspect Level Sentiment Classification with Deep Memory Network." Conference on Empirical Methods in Natural Language Processing 2016:214-224. [[pdf]](https://arxiv.org/pdf/1605.08900)

### IAN ([ian.py](./models/ian.py))

Ma, Dehong, et al. "Interactive Attention Networks for Aspect-Level Sentiment Classification." arXiv preprint arXiv:1709.00893 (2017). [[pdf]](https://arxiv.org/pdf/1709.00893)

### ATAE-LSTM ([atae_lstm.py](./models/atae_lstm.py))

Wang, Yequan, Minlie Huang, and Li Zhao. "Attention-based lstm for aspect-level sentiment classification." Proceedings of the 2016 conference on empirical methods in natural language processing. 2016.

### TD-LSTM ([td_lstm.py](./models/td_lstm.py), [tc_lstm.py](./models/tc_lstm.py)) ([official](https://drive.google.com/open?id=17RF8MZs456ov9MDiUYZp0SCGL6LvBQl6))

Tang, Duyu, et al. "Effective LSTMs for Target-Dependent Sentiment Classification." Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. 2016. [[pdf]](https://arxiv.org/pdf/1512.01100)

### LSTM ([lstm.py](./models/lstm.py))

Hochreiter, Sepp, and JÃ¼rgen Schmidhuber. "Long short-term memory." Neural computation 9.8 (1997): 1735-1780. [[pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.676.4320&rep=rep1&type=pdf)]


## Contributors

Thanks goes to these wonderful people:

<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->
<!-- prettier-ignore-start -->
<!-- markdownlint-disable -->
<table>
  <tr>
    <td align="center"><a href="https://github.com/AlbertoPaz"><img src="https://avatars2.githubusercontent.com/u/36967362?v=4" width="100px;" alt=""/><br /><sub><b>Alberto Paz</b></sub></a><br /><a href="https://github.com/songyouwei/ABSA-PyTorch/commits?author=AlbertoPaz" title="Code">ğŸ’»</a></td>
    <td align="center"><a href="http://taojiang0923@gmail.com"><img src="https://avatars0.githubusercontent.com/u/37891032?v=4" width="100px;" alt=""/><br /><sub><b>jiangtao </b></sub></a><br /><a href="https://github.com/songyouwei/ABSA-PyTorch/commits?author=jiangtaojy" title="Code">ğŸ’»</a></td>
    <td align="center"><a href="https://genezc.github.io"><img src="https://avatars0.githubusercontent.com/u/24239326?v=4" width="100px;" alt=""/><br /><sub><b>WhereIsMyHead</b></sub></a><br /><a href="https://github.com/songyouwei/ABSA-PyTorch/commits?author=GeneZC" title="Code">ğŸ’»</a></td>
    <td align="center"><a href="https://github.com/songyouwei"><img src="https://avatars1.githubusercontent.com/u/2573291?v=4" width="100px;" alt=""/><br /><sub><b>songyouwei</b></sub></a><br /><a href="https://github.com/songyouwei/ABSA-PyTorch/commits?author=songyouwei" title="Code">ğŸ’»</a></td>
    <td align="center"><a href="https://github.com/yangheng95"><img src="https://avatars2.githubusercontent.com/u/51735130?v=4" width="100px;" alt=""/><br /><sub><b>YangHeng</b></sub></a><br /><a href="https://github.com/songyouwei/ABSA-PyTorch/commits?author=yangheng95" title="Code">ğŸ’»</a></td>
    <td align="center"><a href="https://github.com/rmarcacini"><img src="https://avatars0.githubusercontent.com/u/40037976?v=4" width="100px;" alt=""/><br /><sub><b>rmarcacini</b></sub></a><br /><a href="https://github.com/songyouwei/ABSA-PyTorch/commits?author=rmarcacini" title="Code">ğŸ’»</a></td>
    <td align="center"><a href="https://github.com/ZhangYikaii"><img src="https://avatars1.githubusercontent.com/u/46623714?v=4" width="100px;" alt=""/><br /><sub><b>Yikai Zhang</b></sub></a><br /><a href="https://github.com/songyouwei/ABSA-PyTorch/commits?author=ZhangYikaii" title="Code">ğŸ’»</a></td>
  </tr>
</table>

<!-- markdownlint-enable -->
<!-- prettier-ignore-end -->
<!-- ALL-CONTRIBUTORS-LIST:END -->

This project follows the [all-contributors](https://github.com/all-contributors/all-contributors) specification. Contributions of any kind welcome!

## Licence

MIT
